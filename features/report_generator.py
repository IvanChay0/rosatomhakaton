import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import re

class ReportGenerator:
    def __init__(self):
        pass
    
    def generate_text_analysis(self, df, original_query):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        if df.empty:
            return self._format_empty_response(original_query)
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            query_type = self._analyze_query_type(original_query)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
            if query_type == "comparison":
                return self._generate_comparison_analysis(df, original_query)
            elif query_type == "ranking":
                return self._generate_ranking_analysis(df, original_query)
            elif query_type == "aggregation":
                return self._generate_aggregation_analysis(df, original_query)
            elif query_type == "trend":
                return self._generate_trend_analysis(df, original_query)
            elif query_type == "distribution":
                return self._generate_distribution_analysis(df, original_query)
            else:
                return self._generate_general_analysis(df, original_query)
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            return self._generate_simple_analysis(df, original_query)
    
    def _analyze_query_type(self, query):
        """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['—Å—Ä–∞–≤–Ω', 'compare', '—Å–æ–ø–æ—Å—Ç–∞–≤', '–ø—Ä–æ—Ç–∏–≤']):
            return "comparison"
        elif any(word in query_lower for word in ['—Ç–æ–ø', '–ª—É—á—à', '–ø–µ—Ä–≤—ã–µ', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ', 'ranking', '—Ä–µ–π—Ç–∏–Ω–≥']):
            return "ranking"
        elif any(word in query_lower for word in ['—Å–∫–æ–ª—å–∫–æ', '—Å—É–º–º', '–æ–±—â', '–≤—Å–µ–≥–æ', '–∏—Ç–æ–≥', 'total', 'sum']):
            return "aggregation"
        elif any(word in query_lower for word in ['—Ç—Ä–µ–Ω–¥', '–¥–∏–Ω–∞–º–∏–∫', '–∏–∑–º–µ–Ω–µ–Ω', '—Ä–æ—Å—Ç', '—Å–Ω–∏–∂–µ–Ω', 'trend']):
            return "trend"
        elif any(word in query_lower for word in ['—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω', '—á–∞—Å—Ç–æ—Ç–∞', '—Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ', 'distribution']):
            return "distribution"
        else:
            return "general"
    
    def _generate_comparison_analysis(self, df, query):
        """–ê–Ω–∞–ª–∏–∑ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–π"""
        analysis = f"# üîç –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        analysis += f"## üìã –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö\n\n"
        analysis += f"‚Ä¢ **–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:** {len(df):,}\n"
        analysis += f"‚Ä¢ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π:** {len(df.columns)}\n\n"
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        categorical_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
        
        if numeric_cols and categorical_cols:
            # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –Ω—É–∂–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ —á–∏—Å–ª–æ
            cat_col = categorical_cols[0]
            num_col = numeric_cols[0]
            
            analysis += f"## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ '{self._translate_column(cat_col)}'\n\n"
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            grouped = df.groupby(cat_col)[num_col].agg(['sum', 'mean', 'count']).round(2)
            grouped = grouped.sort_values('sum', ascending=False)
            
            for category, row in grouped.head(5).iterrows():
                analysis += f"### {category}\n"
                analysis += f"- **–û–±—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(row['sum'])}\n"
                analysis += f"- **–í —Å—Ä–µ–¥–Ω–µ–º:** {self._format_number(row['mean'])}\n"
                analysis += f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π:** {int(row['count'])}\n\n"
            
            if len(grouped) > 5:
                analysis += f"*... –∏ –µ—â–µ {len(grouped) - 5} –∫–∞—Ç–µ–≥–æ—Ä–∏–π*\n\n"
        
        # –ò–Ω—Å–∞–π—Ç—ã
        analysis += f"## üí° –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã\n\n"
        
        insights = []
        if len(numeric_cols) >= 2:
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            col1, col2 = numeric_cols[0], numeric_cols[1]
            corr = df[col1].corr(df[col2])
            
            if abs(corr) > 0.7:
                insights.append(f"–°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É '{self._translate_column(col1)}' –∏ '{self._translate_column(col2)}' ({corr:.2f})")
            elif abs(corr) > 0.3:
                insights.append(f"–£–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É '{self._translate_column(col1)}' –∏ '{self._translate_column(col2)}'")
        
        if categorical_cols:
            main_cat = categorical_cols[0]
            unique_count = df[main_cat].nunique()
            if unique_count <= 10:
                insights.append(f"–î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {unique_count} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        
        if not insights:
            insights.append("–î–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π")
            insights.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏")
        
        for i, insight in enumerate(insights[:3], 1):
            analysis += f"{i}. {insight}\n"
        analysis += "\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        analysis += f"## üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n"
        recommendations = [
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Å—Ç–æ–ª–±—á–∞—Ç—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã)",
            "–ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π",
            "–°—Ä–∞–≤–Ω–∏—Ç–µ –º–µ–¥–∏–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤",
            "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–ø—Ä–æ—Ü–µ–Ω—Ç—ã, –¥–æ–ª–∏)",
            "–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ Excel"
        ]
        
        for i, rec in enumerate(recommendations, 1):
            analysis += f"{i}. {rec}\n"
        
        return self._format_response(analysis)
    
    def _generate_ranking_analysis(self, df, query):
        """–ê–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –∏ —Ç–æ–ø–æ–≤"""
        analysis = f"# üèÜ –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ –∫–∞–∫–æ–º—É –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        categorical_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
        
        if not numeric_cols:
            return self._generate_general_analysis(df, query)
        
        # –í—ã–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —á–∏—Å–ª–æ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        rank_col = numeric_cols[0]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df_sorted = df.sort_values(rank_col, ascending=False).head(10)
        
        analysis += f"## üìä –¢–æ–ø-{len(df_sorted)} –ø–æ '{self._translate_column(rank_col)}'\n\n"
        
        for idx, (_, row) in enumerate(df_sorted.iterrows(), 1):
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏
            description = self._describe_row_for_ranking(row, categorical_cols)
            value = self._format_number(row[rank_col])
            
            medal = ""
            if idx == 1:
                medal = "ü•á "
            elif idx == 2:
                medal = "ü•à "
            elif idx == 3:
                medal = "ü•â "
            
            analysis += f"### {medal}{idx}. {description}\n"
            analysis += f"- **{self._translate_column(rank_col)}:** {value}\n"
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if len(numeric_cols) > 1:
                extra_col = numeric_cols[1]
                extra_value = self._format_number(row[extra_col])
                analysis += f"- **{self._translate_column(extra_col)}:** {extra_value}\n"
            
            analysis += "\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        analysis += f"## üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è\n\n"
        analysis += f"‚Ä¢ **–í—Å–µ–≥–æ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ:** {len(df)} –∑–∞–ø–∏—Å–µ–π\n"
        analysis += f"‚Ä¢ **–õ–∏–¥–µ—Ä (–º–∞–∫—Å–∏–º—É–º):** {self._format_number(df[rank_col].max())}\n"
        analysis += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(df[rank_col].mean())}\n"
        analysis += f"‚Ä¢ **–†–∞–∑—Ä—ã–≤ –ª–∏–¥–µ—Ä–∞ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ:** {self._format_number((df[rank_col].max() / df[rank_col].mean() - 1) * 100)}%\n\n"
        
        # –ò–Ω—Å–∞–π—Ç—ã
        analysis += f"## üí° –ù–∞–±–ª—é–¥–µ–Ω–∏—è\n\n"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
        q75, q25 = np.percentile(df[rank_col].dropna(), [75, 25])
        iqr = q75 - q25
        outliers = df[df[rank_col] > q75 + 1.5 * iqr]
        
        if len(outliers) > 0:
            analysis += f"1. **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã–¥–∞—é—â–∏–µ—Å—è –∑–Ω–∞—á–µ–Ω–∏—è** ({len(outliers)} –∑–∞–ø–∏—Å–µ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ)\n"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if df[rank_col].std() / df[rank_col].mean() > 0.5:
            analysis += f"2. **–í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å** –¥–∞–Ω–Ω—ã—Ö (–∑–Ω–∞—á–µ–Ω–∏—è —Å–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è)\n"
        else:
            analysis += f"2. **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ** —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π\n"
        
        if len(df_sorted) < len(df):
            analysis += f"3. –ü–æ–∫–∞–∑–∞–Ω—ã —Ç–æ–ª—å–∫–æ —Ç–æ–ø-{len(df_sorted)} –∏–∑ {len(df)} –∑–∞–ø–∏—Å–µ–π\n"
        
        analysis += "\n"
        
        return self._format_response(analysis)
    
    def _generate_aggregation_analysis(self, df, query):
        """–ê–Ω–∞–ª–∏–∑ –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        analysis = f"# üßÆ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        # –ù–∞—Ö–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        
        if not numeric_cols:
            # –ï—Å–ª–∏ –Ω–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ—Å—Ç–æ —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            analysis += f"## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Å—á–µ—Ç–∞\n\n"
            analysis += f"‚Ä¢ **–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:** {len(df):,}\n\n"
            
            # –°—á–∏—Ç–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            categorical_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
            if categorical_cols:
                for col in categorical_cols[:2]:
                    unique_count = df[col].nunique()
                    analysis += f"‚Ä¢ **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ '{self._translate_column(col)}':** {unique_count}\n"
            
            return self._format_response(analysis)
        
        analysis += f"## üìà –°—É–º–º–∞—Ä–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏\n\n"
        
        for col in numeric_cols[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
            total = df[col].sum()
            avg = df[col].mean()
            median_val = df[col].median()
            
            analysis += f"### {self._translate_column(col)}\n"
            analysis += f"- **–û–±—â–∞—è —Å—É–º–º–∞:** {self._format_number(total)}\n"
            
            if len(df) > 1:
                analysis += f"- **–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(avg)}\n"
                analysis += f"- **–ú–µ–¥–∏–∞–Ω–∞:** {self._format_number(median_val)}\n"
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
                if len(numeric_cols) > 1 and total > 0:
                    percentage = (total / sum(df[numeric_cols[0]].sum() for _ in numeric_cols[:3])) * 100
                    analysis += f"- **–î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ:** {percentage:.1f}%\n"
            
            analysis += "\n"
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –µ—Å–ª–∏ –µ—Å—Ç—å
        categorical_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
        
        if categorical_cols and len(df) > 5:
            cat_col = categorical_cols[0]
            num_col = numeric_cols[0]
            
            analysis += f"## üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ '{self._translate_column(cat_col)}'\n\n"
            
            grouped = df.groupby(cat_col)[num_col].sum().nlargest(5)
            
            for category, value in grouped.items():
                percentage = (value / df[num_col].sum()) * 100
                analysis += f"- **{category}:** {self._format_number(value)} ({percentage:.1f}%)\n"
            
            analysis += "\n"
        
        # –ò–Ω—Å–∞–π—Ç—ã
        analysis += f"## üí° –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã\n\n"
        
        main_col = numeric_cols[0]
        total = df[main_col].sum()
        
        if total > 1000000:
            analysis += f"1. **–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –æ–±—ä–µ–º** - –æ–±—â–∞—è —Å—É–º–º–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {self._format_number(total)}\n"
        elif total < 1000:
            analysis += f"1. **–ù–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º** –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n"
        
        if len(df) > 100:
            analysis += f"2. **–ë–æ–ª—å—à–∞—è –≤—ã–±–æ—Ä–∫–∞** ({len(df):,} –∑–∞–ø–∏—Å–µ–π) –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å\n"
        elif len(df) < 10:
            analysis += f"2. **–ú–∞–ª–µ–Ω—å–∫–∞—è –≤—ã–±–æ—Ä–∫–∞** - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–±—É—é—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
        if len(df) > 10:
            q1 = df[main_col].quantile(0.25)
            q3 = df[main_col].quantile(0.75)
            iqr = q3 - q1
            outliers = df[(df[main_col] < q1 - 1.5*iqr) | (df[main_col] > q3 + 1.5*iqr)]
            
            if len(outliers) > 0:
                analysis += f"3. **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è** ({len(outliers)} –≤—ã–±—Ä–æ—Å–æ–≤)\n"
        
        analysis += "\n"
        
        return self._format_response(analysis)
    
    def _generate_trend_analysis(self, df, query):
        """–ê–Ω–∞–ª–∏–∑ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –¥–∏–Ω–∞–º–∏–∫–∏"""
        analysis = f"# üìà –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏
        date_cols = []
        for col in df.columns:
            col_lower = str(col).lower()
            if any(keyword in col_lower for keyword in ['date', '–¥–∞—Ç–∞', 'time', '–≤—Ä–µ–º—è', '–≥–æ–¥', '–º–µ—Å—è—Ü', '–¥–µ–Ω—å']):
                date_cols.append(col)
        
        if not date_cols:
            return self._generate_general_analysis(df, query)
        
        date_col = date_cols[0]
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        
        if not numeric_cols:
            return self._generate_general_analysis(df, query)
        
        num_col = numeric_cols[0]
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã
            df_copy = df.copy()
            df_copy['_temp_date'] = pd.to_datetime(df_copy[date_col], errors='coerce')
            df_copy = df_copy.dropna(subset=['_temp_date', num_col])
            
            if len(df_copy) < 2:
                return self._generate_general_analysis(df, query)
            
            df_copy = df_copy.sort_values('_temp_date')
            
            analysis += f"## üìÖ –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞\n\n"
            min_date = df_copy['_temp_date'].min().strftime('%d.%m.%Y')
            max_date = df_copy['_temp_date'].max().strftime('%d.%m.%Y')
            days_count = (df_copy['_temp_date'].max() - df_copy['_temp_date'].min()).days
            
            analysis += f"‚Ä¢ **–ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∞:** {min_date}\n"
            analysis += f"‚Ä¢ **–ö–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞:** {max_date}\n"
            analysis += f"‚Ä¢ **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {days_count} –¥–Ω–µ–π\n"
            analysis += f"‚Ä¢ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö:** {len(df_copy)}\n\n"
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
            analysis += f"## üìä –î–∏–Ω–∞–º–∏–∫–∞ '{self._translate_column(num_col)}'\n\n"
            
            first_value = df_copy.iloc[0][num_col]
            last_value = df_copy.iloc[-1][num_col]
            total_change = last_value - first_value
            percent_change = (total_change / first_value * 100) if first_value != 0 else 0
            
            trend_emoji = "‚ÜóÔ∏è" if total_change > 0 else "‚ÜòÔ∏è" if total_change < 0 else "‚û°Ô∏è"
            trend_word = "—Ä–æ—Å—Ç" if total_change > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ" if total_change < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"
            
            analysis += f"‚Ä¢ **–ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(first_value)}\n"
            analysis += f"‚Ä¢ **–ö–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(last_value)}\n"
            analysis += f"‚Ä¢ **–ò–∑–º–µ–Ω–µ–Ω–∏–µ:** {trend_emoji} {self._format_number(total_change)} ({percent_change:+.1f}%)\n"
            analysis += f"‚Ä¢ **–¢—Ä–µ–Ω–¥:** {trend_word}\n\n"
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            if len(df_copy) >= 3:
                # –î–µ–ª–∏–º –Ω–∞ —Ç—Ä–µ—Ç–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                third = len(df_copy) // 3
                first_third_avg = df_copy.head(third)[num_col].mean()
                last_third_avg = df_copy.tail(third)[num_col].mean()
                third_change = ((last_third_avg - first_third_avg) / first_third_avg * 100) if first_third_avg != 0 else 0
                
                analysis += f"### üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–æ–≤\n\n"
                analysis += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ –≤ –Ω–∞—á–∞–ª–µ –ø–µ—Ä–∏–æ–¥–∞:** {self._format_number(first_third_avg)}\n"
                analysis += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ –≤ –∫–æ–Ω—Ü–µ –ø–µ—Ä–∏–æ–¥–∞:** {self._format_number(last_third_avg)}\n"
                analysis += f"‚Ä¢ **–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ:** {third_change:+.1f}%\n\n"
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            analysis += f"## üìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏\n\n"
            analysis += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {self._format_number(df_copy[num_col].mean())}\n"
            analysis += f"‚Ä¢ **–ú–µ–¥–∏–∞–Ω–∞:** {self._format_number(df_copy[num_col].median())}\n"
            analysis += f"‚Ä¢ **–ú–∏–Ω–∏–º—É–º:** {self._format_number(df_copy[num_col].min())}\n"
            analysis += f"‚Ä¢ **–ú–∞–∫—Å–∏–º—É–º:** {self._format_number(df_copy[num_col].max())}\n"
            analysis += f"‚Ä¢ **–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ):** {self._format_number(df_copy[num_col].std())}\n\n"
            
            # –ò–Ω—Å–∞–π—Ç—ã
            analysis += f"## üí° –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\n\n"
            
            insights = []
            
            if abs(percent_change) > 20:
                insights.append(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π {trend_word} –∑–∞ –ø–µ—Ä–∏–æ–¥ ({percent_change:+.1f}%)")
            elif abs(percent_change) > 5:
                insights.append(f"–£–º–µ—Ä–µ–Ω–Ω—ã–π {trend_word} –∑–∞ –ø–µ—Ä–∏–æ–¥")
            else:
                insights.append("–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
            
            if df_copy[num_col].std() / df_copy[num_col].mean() > 0.3:
                insights.append("–í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö")
            
            if len(df_copy) >= 10:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å/–ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å
                insights.append(f"–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(df_copy)} —Ç–æ—á–µ–∫) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            
            for i, insight in enumerate(insights, 1):
                analysis += f"{i}. {insight}\n"
            
            analysis += "\n"
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            analysis += f"## üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n"
            recommendations = [
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–∞",
                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞–∑–±–∏–≤–∫—É –ø–æ –º–µ—Å—è—Ü–∞–º/–∫–≤–∞—Ä—Ç–∞–ª–∞–º",
                "–ò—Å–∫–ª—é—á–∏—Ç–µ –≤—ã–±—Ä–æ—Å—ã –¥–ª—è –±–æ–ª–µ–µ —á–µ—Ç–∫–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞",
                "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥–∞–Ω–Ω—ã—Ö",
                "–°–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥"
            ]
            
            for i, rec in enumerate(recommendations, 1):
                analysis += f"{i}. {rec}\n"
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–∞: {e}")
            return self._generate_general_analysis(df, query)
        
        return self._format_response(analysis)
    
    def _generate_distribution_analysis(self, df, query):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        analysis = f"# üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        analysis += f"## üìã –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö\n\n"
        analysis += f"‚Ä¢ **–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:** {len(df):,}\n"
        analysis += f"‚Ä¢ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π:** {len(df.columns)}\n\n"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        
        if numeric_cols:
            analysis += f"## üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π\n\n"
            
            for col in numeric_cols[:2]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 2 –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
                values = df[col].dropna()
                
                if len(values) == 0:
                    continue
                
                analysis += f"### {self._translate_column(col)}\n"
                analysis += f"‚Ä¢ **–î–∏–∞–ø–∞–∑–æ–Ω:** –æ—Ç {self._format_number(values.min())} –¥–æ {self._format_number(values.max())}\n"
                analysis += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ:** {self._format_number(values.mean())}\n"
                analysis += f"‚Ä¢ **–ú–µ–¥–∏–∞–Ω–∞:** {self._format_number(values.median())}\n"
                
                # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è
                skewness = values.skew()
                if abs(skewness) > 1:
                    skew_type = "—Å–∏–ª—å–Ω–æ —Å–∫–æ—à–µ–Ω–Ω–æ–µ" if skewness > 0 else "—Å–∏–ª—å–Ω–æ –ª–µ–≤–æ—Å–∫–æ—à–µ–Ω–Ω–æ–µ"
                elif abs(skewness) > 0.5:
                    skew_type = "—Å–∫–æ—à–µ–Ω–Ω–æ–µ" if skewness > 0 else "–ª–µ–≤–æ—Å–∫–æ—à–µ–Ω–Ω–æ–µ"
                else:
                    skew_type = "—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ"
                
                analysis += f"‚Ä¢ **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** {skew_type} (–∞—Å–∏–º–º–µ—Ç—Ä–∏—è: {skewness:.2f})\n"
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏
                percentiles = values.quantile([0.25, 0.5, 0.75])
                analysis += f"‚Ä¢ **25-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å:** {self._format_number(percentiles[0.25])}\n"
                analysis += f"‚Ä¢ **75-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å:** {self._format_number(percentiles[0.75])}\n\n"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        categorical_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
        
        if categorical_cols:
            analysis += f"## üè∑Ô∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n\n"
            
            for col in categorical_cols[:2]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 2 –∫–æ–ª–æ–Ω–∫–∞–º–∏
                value_counts = df[col].value_counts()
                total = len(df[col].dropna())
                
                analysis += f"### {self._translate_column(col)}\n"
                analysis += f"‚Ä¢ **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:** {len(value_counts)}\n"
                analysis += f"‚Ä¢ **–°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è:** {value_counts.index[0]} ({value_counts.iloc[0] / total * 100:.1f}%)\n"
                
                if len(value_counts) <= 10:
                    analysis += f"‚Ä¢ **–í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:**\n"
                    for value, count in value_counts.head(5).items():
                        percentage = count / total * 100
                        analysis += f"  - {value}: {count} ({percentage:.1f}%)\n"
                else:
                    analysis += f"‚Ä¢ **–¢–æ–ø-5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π:**\n"
                    for value, count in value_counts.head(5).items():
                        percentage = count / total * 100
                        analysis += f"  - {value}: {count} ({percentage:.1f}%)\n"
                
                analysis += "\n"
        
        # –ò–Ω—Å–∞–π—Ç—ã
        analysis += f"## üí° –í—ã–≤–æ–¥—ã\n\n"
        
        insights = []
        
        if numeric_cols:
            main_num = numeric_cols[0]
            cv = df[main_num].std() / df[main_num].mean() * 100 if df[main_num].mean() != 0 else 0
            
            if cv > 100:
                insights.append("–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö")
            elif cv > 50:
                insights.append("–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å –∑–Ω–∞—á–µ–Ω–∏–π")
            elif cv > 20:
                insights.append("–£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å")
            else:
                insights.append("–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        if categorical_cols:
            main_cat = categorical_cols[0]
            top_percentage = df[main_cat].value_counts().iloc[0] / len(df) * 100
            
            if top_percentage > 50:
                insights.append(f"–î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è ({top_percentage:.0f}% –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π)")
            elif top_percentage < 20:
                insights.append("–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        
        if len(df) > 1000:
            insights.append("–ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å")
        
        for i, insight in enumerate(insights[:3], 1):
            analysis += f"{i}. {insight}\n"
        
        analysis += "\n"
        
        return self._format_response(analysis)
    
    def _generate_general_analysis(self, df, query):
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ª—é–±—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        analysis = f"# üìã –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        
        analysis += f"## üìä –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n"
        analysis += f"‚Ä¢ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π:** {len(df):,}\n"
        analysis += f"‚Ä¢ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫:** {len(df.columns)}\n"
        
        # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        numeric_count = sum(1 for col in df.columns if pd.api.types.is_numeric_dtype(df[col]))
        text_count = len(df.columns) - numeric_count
        
        analysis += f"‚Ä¢ **–ß–∏—Å–ª–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π:** {numeric_count}\n"
        analysis += f"‚Ä¢ **–¢–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π:** {text_count}\n\n"
        
        # –û–±–∑–æ—Ä –∫–æ–ª–æ–Ω–æ–∫
        analysis += f"## üìë –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n\n"
        
        for i, col in enumerate(df.columns[:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫
            col_type = "—á–∏—Å–ª–æ–≤–æ–π" if pd.api.types.is_numeric_dtype(df[col]) else "—Ç–µ–∫—Å—Ç–æ–≤—ã–π"
            unique_count = df[col].nunique()
            
            analysis += f"{i}. **{self._translate_column(col)}** ({col_type})\n"
            analysis += f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_count}\n"
            
            if col_type == "—á–∏—Å–ª–æ–≤–æ–π":
                analysis += f"   - –î–∏–∞–ø–∞–∑–æ–Ω: {self._format_number(df[col].min())} - {self._format_number(df[col].max())}\n"
            elif unique_count <= 5:
                analysis += f"   - –ü—Ä–∏–º–µ—Ä—ã: {', '.join(map(str, df[col].unique()[:3]))}\n"
            
            analysis += "\n"
        
        if len(df.columns) > 5:
            analysis += f"*... –∏ –µ—â–µ {len(df.columns) - 5} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π*\n\n"
        
        # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        if len(df) > 0:
            analysis += f"## üëÅÔ∏è –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–µ–π\n\n"
            
            for i in range(min(3, len(df))):
                row = df.iloc[i]
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–æ–ª–æ–Ω–∫–∏
                sample_cols = df.columns[:3]
                sample_text = []
                
                for col in sample_cols:
                    value = row[col]
                    if pd.isna(value):
                        sample_text.append(f"{self._translate_column(col)}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    else:
                        val_str = str(value)
                        if len(val_str) > 30:
                            val_str = val_str[:27] + "..."
                        sample_text.append(f"{self._translate_column(col)}: {val_str}")
                
                analysis += f"**–ó–∞–ø–∏—Å—å {i+1}:** {', '.join(sample_text)}\n\n"
        
        # –ò–Ω—Å–∞–π—Ç—ã
        analysis += f"## üí° –ß—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏?\n\n"
        
        suggestions = []
        
        if numeric_count >= 2:
            suggestions.append("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É —á–∏—Å–ª–æ–≤—ã–º–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏")
        
        if text_count >= 1 and numeric_count >= 1:
            suggestions.append("–°—Ä–∞–≤–Ω–∏—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        
        if len(df) > 50:
            suggestions.append("–ü—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
        
        suggestions.append("–°–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è")
        suggestions.append("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        
        for i, suggestion in enumerate(suggestions[:5], 1):
            analysis += f"{i}. {suggestion}\n"
        
        analysis += "\n"
        
        return self._format_response(analysis)
    
    def _generate_simple_analysis(self, df, query):
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–æ–∫"""
        analysis = f"# üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—Ä–æ—Å–∞\n\n"
        analysis += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        analysis += f"‚úÖ **–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã**\n\n"
        analysis += f"‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: **{len(df):,}**\n"
        analysis += f"‚Ä¢ –ö–æ–ª–æ–Ω–∫–∏: {', '.join([self._translate_column(col) for col in df.columns[:5]])}\n"
        
        if len(df.columns) > 5:
            analysis += f"  ... –∏ –µ—â–µ {len(df.columns) - 5} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π\n"
        
        # –°—É–º–º–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        if numeric_cols:
            analysis += f"\n**–°—É–º–º–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**\n"
            for col in numeric_cols[:2]:
                total = df[col].sum()
                if abs(total) > 0:
                    analysis += f"‚Ä¢ {self._translate_column(col)}: **{self._format_number(total)}**\n"
        
        analysis += f"\nüí° **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∫–ª–∞–¥–∫—É '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è' –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.**"
        
        return self._format_response(analysis)
    
    def _format_empty_response(self, query):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        response = f"# üì≠ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞\n\n"
        response += f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n"
        response += f"‚ùå **–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n\n"
        response += f"–ó–∞–ø—Ä–æ—Å –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ:\n\n"
        response += f"1. **–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö**, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º\n"
        response += f"2. **–û—à–∏–±–∫–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ** –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ö\n"
        response += f"3. **–ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º** –∫ –¥–∞–Ω–Ω—ã–º\n\n"
        response += f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n\n"
        response += f"‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞\n"
        response += f"‚Ä¢ –£–ø—Ä–æ—Å—Ç–∏—Ç–µ —É—Å–ª–æ–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n"
        response += f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ –≤—ã—à–µ\n"
        
        return self._format_response(response)
    
    def _format_response(self, text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç—Å—Ç—É–ø–∞–º–∏ –∏ —Ä–∞–∑–º–µ—Ç–∫–æ–π"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        lines = text.strip().split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.rstrip()
            if line.strip() == '' and formatted_lines and formatted_lines[-1].strip() == '':
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            formatted_lines.append(line)
        
        return '\n'.join(formatted_lines)
    
    def _describe_row_for_ranking(self, row, categorical_cols):
        """–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥–∞"""
        if not categorical_cols:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            values = []
            for i in range(min(2, len(row))):
                val = row.iloc[i]
                if not pd.isna(val):
                    values.append(str(val)[:20])
            return ", ".join(values) if values else "–ó–∞–ø–∏—Å—å"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
        desc_parts = []
        for col in categorical_cols[:2]:
            if col in row.index and not pd.isna(row[col]):
                val = str(row[col])
                desc_parts.append(val[:25])
        
        return ", ".join(desc_parts) if desc_parts else "–ó–∞–ø–∏—Å—å"
    
    def _translate_column(self, column_name):
        """–ü–µ—Ä–µ–≤–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
        translations = {
            'project_id': 'ID –ø—Ä–æ–µ–∫—Ç–∞',
            'project_name': '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',
            'budget': '–ë—é–¥–∂–µ—Ç',
            'revenue': '–í—ã—Ä—É—á–∫–∞',
            'salary': '–ó–∞—Ä–ø–ª–∞—Ç–∞',
            'employee_id': 'ID —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞',
            'first_name': '–ò–º—è',
            'last_name': '–§–∞–º–∏–ª–∏—è',
            'department': '–û—Ç–¥–µ–ª',
            'position': '–î–æ–ª–∂–Ω–æ—Å—Ç—å',
            'hire_date': '–î–∞—Ç–∞ –ø—Ä–∏–µ–º–∞',
            'performance_score': '–û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
            'start_date': '–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞',
            'end_date': '–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è',
            'status': '–°—Ç–∞—Ç—É—Å',
            'manager_id': 'ID —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è',
            'equipment_id': 'ID –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è',
            'equipment_name': '–ù–∞–∑–≤–∞–Ω–∏–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è',
            'type': '–¢–∏–ø',
            'purchase_date': '–î–∞—Ç–∞ –ø–æ–∫—É–ø–∫–∏',
            'maintenance_date': '–î–∞—Ç–∞ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è',
            'cost': '–°—Ç–æ–∏–º–æ—Å—Ç—å',
            'production_id': 'ID –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
            'date': '–î–∞—Ç–∞',
            'product_name': '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞',
            'quantity': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
            'incident_id': 'ID –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞',
            'description': '–û–ø–∏—Å–∞–Ω–∏–µ',
            'severity': '–£—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏',
            'resolved': '–†–µ—à–µ–Ω',
            'resolution_time_hours': '–í—Ä–µ–º—è —Ä–µ—à–µ–Ω–∏—è (—á–∞—Å—ã)',
            'total_revenue': '–û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞',
            'employee_count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤',
            'average_salary': '–°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞',
            'quantity': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
            'price': '–¶–µ–Ω–∞',
            'amount': '–°—É–º–º–∞',
            'value': '–ó–Ω–∞—á–µ–Ω–∏–µ',
            'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
            'sum': '–°—É–º–º–∞',
            'avg': '–°—Ä–µ–¥–Ω–µ–µ',
            'min': '–ú–∏–Ω–∏–º—É–º',
            'max': '–ú–∞–∫—Å–∏–º—É–º'
        }
        
        if isinstance(column_name, str):
            return translations.get(column_name, column_name.replace('_', ' ').title())
        return str(column_name)
    
    def _format_number(self, value, decimals=None):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª"""
        if pd.isna(value):
            return "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        
        try:
            value = float(value)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            if decimals is None:
                if value == 0:
                    return "0"
                elif abs(value) < 0.01:
                    decimals = 4
                elif abs(value) < 1:
                    decimals = 3
                elif abs(value) < 100:
                    decimals = 2
                elif abs(value) < 1000:
                    decimals = 1
                else:
                    decimals = 0
            else:
                decimals = int(decimals)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞
            if abs(value) >= 1_000_000_000:
                formatted = f"{value/1_000_000_000:,.{max(0, decimals-1)}f}".rstrip('0').rstrip('.')
                return f"{formatted} –º–ª—Ä–¥"
            elif abs(value) >= 1_000_000:
                formatted = f"{value/1_000_000:,.{max(0, decimals-1)}f}".rstrip('0').rstrip('.')
                return f"{formatted} –º–ª–Ω"
            elif abs(value) >= 1_000:
                formatted = f"{value/1_000:,.{max(0, decimals-1)}f}".rstrip('0').rstrip('.')
                return f"{formatted} —Ç—ã—Å"
            else:
                if decimals == 0:
                    return f"{int(value):,}".replace(',', ' ')
                else:
                    formatted = f"{value:,.{decimals}f}".rstrip('0').rstrip('.')
                    return formatted.replace(',', ' ')
                    
        except (ValueError, TypeError):
            return str(value)
    
def generate_report(self, report_type='summary', filters=None):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –ë–î
    # –°–µ–π—á–∞—Å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
    
    report_data = {
        'title': f'–û—Ç—á–µ—Ç {report_type}',
        'date': datetime.now().strftime('%d.%m.%Y %H:%M'),
        'type': report_type,
        'metrics': {
            '–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ': '–î–∞',
            '–¢–∏–ø –æ—Ç—á–µ—Ç–∞': report_type,
            '–§–æ—Ä–º–∞—Ç': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π'
        },
        'data': [
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–ü—Ä–∏–º–µ—Ä 1', '–ó–Ω–∞—á–µ–Ω–∏–µ': 100},
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–ü—Ä–∏–º–µ—Ä 2', '–ó–Ω–∞—á–µ–Ω–∏–µ': 200},
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–ü—Ä–∏–º–µ—Ä 3', '–ó–Ω–∞—á–µ–Ω–∏–µ': 300}
        ],
        'columns': ['–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å', '–ó–Ω–∞—á–µ–Ω–∏–µ'],
        'analysis': '–≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç. –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—É–¥—É—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã.',
        'recommendations': [
            '–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ —Å —Ä–µ–∞–ª—å–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö',
            '–ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤',
            '–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ç–∏–ø–æ–≤ –æ—Ç—á–µ—Ç–æ–≤'
        ]
    }
    
    return json.dumps(report_data, ensure_ascii=False, indent=2)

    